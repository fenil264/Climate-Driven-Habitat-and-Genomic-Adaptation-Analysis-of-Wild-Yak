{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bb1643-2d11-490b-a932-59ddcb18433d",
   "metadata": {},
   "source": [
    "# 1) **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a399b57-448b-4c58-b757-10d68557a981",
   "metadata": {},
   "source": [
    "## 1.1) Occurence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63de5c-59f0-4891-95e0-b413435ad9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the target species by its scientific name (Wild Yak only)\n",
    "species_name = \"Bos mutus\"\n",
    "\n",
    "# Base URL endpoints for the GBIF API\n",
    "species_api = \"https://api.gbif.org/v1/species/match?name=\"\n",
    "occurrence_api = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "\n",
    "# Initialize a container to store all occurrence records\n",
    "all_records = []\n",
    "\n",
    "# Send a GET request to retrieve the GBIF species key (usageKey) using the scientific name\n",
    "resp = requests.get(species_api + requests.utils.requote_uri(species_name))\n",
    "data = resp.json()\n",
    "if \"usageKey\" in data:\n",
    "    species_key = data[\"usageKey\"]\n",
    "    print(f\"GBIF species key for {species_name}: {species_key}\")\n",
    "else:\n",
    "    raise ValueError(f\"Species key not found for {species_name}\")\n",
    "\n",
    "# Paginate through occurrence records, with a maximum of 300 records per page\n",
    "offset = 0\n",
    "while True:\n",
    "    params = {\n",
    "        \"taxonKey\": species_key,\n",
    "        \"hasCoordinate\": \"true\",\n",
    "        \"limit\": 300,\n",
    "        \"offset\": offset\n",
    "        # Note: Filters for year and occurrenceStatus have been removed to maximize results\n",
    "    }\n",
    "    res = requests.get(occurrence_api, params=params)\n",
    "    res.raise_for_status()\n",
    "    results = res.json()\n",
    "\n",
    "    # Exit the loop if no results are returned or if there are no more records\n",
    "    if \"results\" not in results or not results[\"results\"]:\n",
    "        break\n",
    "\n",
    "    # Process each occurrence record and extract the relevant fields\n",
    "    for rec in results[\"results\"]:\n",
    "        lat = rec.get(\"decimalLatitude\")\n",
    "        lon = rec.get(\"decimalLongitude\")\n",
    "        date = rec.get(\"eventDate\") or rec.get(\"year\")\n",
    "        source = rec.get(\"datasetName\")\n",
    "\n",
    "        # Only include records that have both latitude and longitude information\n",
    "        if lat is not None and lon is not None:\n",
    "            all_records.append({\n",
    "                \"scientificName\": species_name,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"date\": date,\n",
    "                \"source\": source\n",
    "            })\n",
    "\n",
    "    offset += 300\n",
    "    if results.get(\"endOfRecords\"):\n",
    "        break\n",
    "\n",
    "# Create a DataFrame from the collected records\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# Remove duplicate records based on latitude, longitude, and date\n",
    "df.drop_duplicates(subset=[\"latitude\", \"longitude\", \"date\"], inplace=True)\n",
    "\n",
    "# Remove any records that have missing latitude or longitude values\n",
    "df.dropna(subset=[\"latitude\", \"longitude\"], inplace=True)\n",
    "\n",
    "# Export the cleaned data to a CSV file\n",
    "df.to_csv(\"wild_yak_occurrences.csv\", index=False)\n",
    "print(f\"Saved {len(df)} Wild Yak occurrence records to wild_yak_occurrences.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae9565-5839-4e64-ae53-24c437eb0a77",
   "metadata": {},
   "source": [
    "## 1.2) Weather Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79cf1e-ff64-44bf-b21d-3f2e52039aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Define the output directory where all climate data will be stored.\n",
    "output_dir = \"climate_data\"                  # Base folder for all data\n",
    "\n",
    "# Define the variables to download:\n",
    "# ppt: precipitation, tmin: minimum temperature, tmax: maximum temperature.\n",
    "variables = [\"ppt\", \"tmin\", \"tmax\"]\n",
    "\n",
    "# Define the range of years to download.\n",
    "# Note: The comment states \"years 1990 through 2020 inclusive\" but the code uses years 2009 to 2024.\n",
    "years = range(2009, 2025)\n",
    "\n",
    "# TerraClimate file URL template (provides NetCDF files)\n",
    "base_url = \"http://thredds.northwestknowledge.net:8080/thredds/fileServer/TERRACLIMATE_ALL/data\"\n",
    "\n",
    "# Create the base output directory if it does not already exist.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each climate variable.\n",
    "for var in variables:\n",
    "    # Define the zip file name for the current variable.\n",
    "    zip_filename = os.path.join(output_dir, f\"{var}_1990_2020.zip\")\n",
    "\n",
    "    # Create a new zip file for this variable.\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Process each year in the specified range.\n",
    "        for year in years:\n",
    "            # Construct the file name for the TerraClimate data file.\n",
    "            filename = f\"TerraClimate_{var}_{year}.nc\"\n",
    "\n",
    "            # Define a temporary file path for downloading the file.\n",
    "            temp_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Skip downloading if the file is already added to the ZIP archive.\n",
    "            if filename in zipf.namelist():\n",
    "                print(f\"{filename} already in ZIP, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Construct the full file URL for downloading.\n",
    "            file_url = f\"{base_url}/TerraClimate_{var}_{year}.nc\"\n",
    "            print(f\"Downloading {filename} ...\")\n",
    "\n",
    "            # Download the file using streaming mode to handle large files.\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # Write the downloaded content in chunks to the temporary file.\n",
    "                with open(temp_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"Downloaded {filename}\")\n",
    "\n",
    "                # Add the downloaded file to the ZIP archive.\n",
    "                zipf.write(temp_path, arcname=filename)\n",
    "                print(f\"Added {filename} to ZIP archive.\")\n",
    "\n",
    "                # Remove the temporary file after adding it to the ZIP archive.\n",
    "                os.remove(temp_path)\n",
    "            else:\n",
    "                print(f\"Failed to download {file_url} (status code {response.status_code})\")\n",
    "\n",
    "    print(f\"All data for {var} saved to {zip_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c9826-306b-4042-b55a-6e9220625ac5",
   "metadata": {},
   "source": [
    "## 1.3) Elevation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df1126-8dea-4005-a70e-d16bad735428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the input folder path where the exported Earth Engine files are located in Google Drive\n",
    "input_folder = '/content/drive/MyDrive/EarthEngineExports'\n",
    "\n",
    "# Search for all files with a .tif extension in the input folder\n",
    "tif_files = glob.glob(os.path.join(input_folder, \"*.tif\"))\n",
    "\n",
    "# Print out the total number of TIF files found\n",
    "print(f\"Found {len(tif_files)} TIF files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d698daf-3d36-4e73-9e6c-e9cfae2e0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open each TIF file and store the opened raster file objects in a list.\n",
    "src_files_to_mosaic = [rasterio.open(fp) for fp in tif_files]\n",
    "\n",
    "# Merge all the raster files into one mosaic.\n",
    "mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "# Copy metadata from the first raster file and update it with new properties from the merged mosaic.\n",
    "out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "out_meta.update({\n",
    "    \"driver\": \"GTiff\",              # Use GeoTIFF format for the output.\n",
    "    \"height\": mosaic.shape[1],      # Set the output height based on the mosaic.\n",
    "    \"width\": mosaic.shape[2],       # Set the output width based on the mosaic.\n",
    "    \"transform\": out_trans,         # Update the transform information.\n",
    "    \"count\": 1                      # Specify that there is only one band.\n",
    "})\n",
    "\n",
    "# Define the output path for the merged raster.\n",
    "output_path = '/content/drive/MyDrive/merged_asia_elevation.tif'\n",
    "\n",
    "# Write the mosaic to a new GeoTIFF file using the updated metadata.\n",
    "with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "    dest.write(mosaic)\n",
    "\n",
    "# Print a confirmation message indicating that the merged raster was saved.\n",
    "print(f\"Merged raster saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679c809-669e-4683-8816-d5428292319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Step 1: Build a virtual raster (VRT) that references all the individual elevation tiles.\n",
    "tiles_dir = r\"C:\\\\Users\\\\FENIL\\\\Downloads\\\\Elevation\"\n",
    "tif_files = glob.glob(os.path.join(tiles_dir, \"*.tif\"))\n",
    "\n",
    "# Create a VRT file that aggregates all the TIFF files from the specified directory.\n",
    "vrt_path = \"asia_elevation.vrt\"\n",
    "gdal.BuildVRT(vrt_path, tif_files)\n",
    "\n",
    "# Step 2: Convert the VRT into a single GeoTIFF.\n",
    "# Define the bounding box for the Asia region.\n",
    "# Note: gdal.Translate requires the window in the following order:\n",
    "# [min_x (left), max_y (top), max_x (right), min_y (bottom)].\n",
    "asia_bounds = [25, 60, 150, -10]  # Adjust these boundaries if needed.\n",
    "\n",
    "output_tif = \"asia_elevation_merged.tif\"\n",
    "\n",
    "# Set translation options:\n",
    "# - TILED=YES creates internal tiling for efficient access.\n",
    "# - COMPRESS=DEFLATE applies compression to reduce file size.\n",
    "# - BIGTIFF=YES allows for the creation of files larger than 4GB if necessary.\n",
    "translate_options = gdal.TranslateOptions(\n",
    "    projWin=asia_bounds,  # Clip the data to the extent of Asia.\n",
    "    creationOptions=[\"TILED=YES\", \"COMPRESS=DEFLATE\", \"BIGTIFF=YES\"]\n",
    ")\n",
    "\n",
    "# Generate the final GeoTIFF by translating the VRT with the specified options.\n",
    "gdal.Translate(output_tif, vrt_path, options=translate_options)\n",
    "\n",
    "# Print confirmation that the merged GeoTIFF has been created.\n",
    "print(f\"GeoTIFF created: {output_tif}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8286980-bed0-408a-9f6e-b96dffb9f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import xarray as xr\n",
    "\n",
    "# === USER SETTINGS ===\n",
    "elevation_path = r\"asia_elevation_merged.tif\"  # Path to the large original elevation raster\n",
    "ref_nc_path = r\"C:\\\\Users\\\\FENIL\\\\Downloads\\\\climate_data\\\\ppt_1990_2020\\\\TerraClimate_ppt_2009.nc\"  # Path to a sample climate NetCDF file (any year works)\n",
    "resampled_output_path = r\"elevation_resampled_to_climate.tif\"  # Output path for the resampled elevation raster\n",
    "\n",
    "print(\"Step 1: Loading reference climate grid...\")\n",
    "# Open the NetCDF file with xarray to access the climate grid\n",
    "ref_nc = xr.open_dataset(ref_nc_path)\n",
    "# Select the first month's data from the 'ppt' variable to use as a reference grid\n",
    "ref_grid = ref_nc['ppt'].isel(time=0)\n",
    "\n",
    "# Retrieve the shape (number of rows and columns) and geographic bounds (longitude and latitude extents) of the reference grid\n",
    "target_height, target_width = ref_grid.shape\n",
    "min_lon, max_lon = float(ref_grid.lon.min()), float(ref_grid.lon.max())\n",
    "min_lat, max_lat = float(ref_grid.lat.min()), float(ref_grid.lat.max())\n",
    "\n",
    "print(f\"Climate raster size: {target_height} rows × {target_width} cols\")\n",
    "print(f\"Geographic extent: lon {min_lon} to {max_lon}, lat {min_lat} to {max_lat}\")\n",
    "\n",
    "# Create a transform object for the output grid based on the bounds and dimensions of the reference climate grid\n",
    "target_transform = rasterio.transform.from_bounds(\n",
    "    min_lon, min_lat, max_lon, max_lat,\n",
    "    target_width, target_height\n",
    ")\n",
    "\n",
    "print(\"\\nStep 2: Opening original elevation raster...\")\n",
    "# Open the original elevation raster\n",
    "with rasterio.open(elevation_path) as src:\n",
    "    print(f\"Original raster CRS: {src.crs}\")\n",
    "    print(f\"Original size: {src.height} rows × {src.width} cols\")\n",
    "\n",
    "    # Prepare output profile by copying the original metadata and updating it for the new grid\n",
    "    profile = src.profile.copy()\n",
    "    profile.update({\n",
    "        \"height\": target_height,       # Set the number of rows to match the reference grid\n",
    "        \"width\": target_width,         # Set the number of columns to match the reference grid\n",
    "        \"transform\": target_transform, # Set the new transform based on the reference grid bounds\n",
    "        \"compress\": 'lzw',             # Use LZW compression for the output file\n",
    "        \"dtype\": 'float32'             # Set the data type of the output file\n",
    "    })\n",
    "\n",
    "    print(\"\\nStep 3: Creating output file and reprojecting...\")\n",
    "    # Create the output raster file and perform the reprojection/resampling\n",
    "    with rasterio.open(resampled_output_path, \"w\", **profile) as dst:\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),         # Use the first band of the source\n",
    "            destination=rasterio.band(dst, 1),      # Write to the first band of the destination\n",
    "            src_transform=src.transform,            # Original source transform\n",
    "            src_crs=src.crs,                        # Original source CRS\n",
    "            dst_transform=target_transform,         # New target transform\n",
    "            dst_crs=src.crs,                        # Keeping the same CRS as the source\n",
    "            resampling=Resampling.bilinear           # Use bilinear resampling for smoother results\n",
    "        )\n",
    "\n",
    "print(f\"\\nDone! Resampled elevation saved to:\\n{resampled_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b30cff-0947-4fa2-8d89-65558e883062",
   "metadata": {},
   "source": [
    "# 2) SDM for Wild Yak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69471b6a-05e4-4a3c-af89-595765b1217f",
   "metadata": {},
   "source": [
    "## 2.1) Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1cb03-98a9-4e76-8c1f-7b39388f34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "# Define a custom function to extract the year from a date string, even when the format is inconsistent\n",
    "def extract_year(date_str):\n",
    "    try:\n",
    "        # Try parsing the date using dateutil, which can handle many common date formats\n",
    "        return parser.parse(str(date_str), fuzzy=True).year\n",
    "    except:\n",
    "        try:\n",
    "            # If parsing fails, check if the string is just a year (e.g., \"2015\")\n",
    "            if len(str(date_str).strip()) == 4 and str(date_str).isdigit():\n",
    "                return int(date_str)\n",
    "        except:\n",
    "            pass\n",
    "        # Return None if the date could not be parsed\n",
    "        return None\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\FENIL\\\\Downloads\\\\final_wild_yak_occurrences.csv')\n",
    "\n",
    "# Apply the year extraction function to the 'date' column and create a new 'year' column\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "\n",
    "# Remove the original 'date' column if it's no longer needed\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('wild_yak_occurrences_years_cleaned.csv', index=False)\n",
    "\n",
    "# Print the first 15 rows to verify the results\n",
    "print(df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6cad3-43c9-403c-9bc8-3d6a63e790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file.\n",
    "df = pd.read_csv(\"wild_yak_Final.csv\")\n",
    "\n",
    "# Exclude records where the 'year' is 1905.\n",
    "df_cleaned = df[df['year'] != 1905]\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file.\n",
    "df_cleaned.to_csv(\"wild_yak_Final_cleaned.csv\", index=False)\n",
    "\n",
    "# Inform that all records from 1905 have been removed.\n",
    "print(\"Removed all records from 1905. New file saved as 'wild_yak_Final_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44421c-e290-40a9-b19e-0c14bd6e6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from geopy.distance import distance\n",
    "from geopy.point import Point\n",
    "\n",
    "# Function to apply a random spatial adjustment (jitter) to a point within a specified radius in kilometers.\n",
    "def jitter_point(lat, lon, max_distance_km):\n",
    "    # Generate a random direction (bearing) between 0 and 360 degrees.\n",
    "    bearing = random.uniform(0, 360)\n",
    "    # Generate a random distance up to the maximum specified distance.\n",
    "    jitter_distance = random.uniform(0, max_distance_km)\n",
    "    # Define the original point using its latitude and longitude.\n",
    "    origin = Point(lat, lon)\n",
    "    # Calculate the destination point given the random distance and bearing.\n",
    "    destination = distance(kilometers=jitter_distance).destination(origin, bearing)\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "# Function to create multiple jittered records for each original record in a dataset.\n",
    "def generate_multiple_jittered_records(df, lat_col='latitude', lon_col='longitude', jitter_per_record=10, max_distance_km=1):\n",
    "    jittered_records = []\n",
    "\n",
    "    # Iterate over each record in the original DataFrame.\n",
    "    for _, row in df.iterrows():\n",
    "        # Generate the specified number of jittered records for the current record.\n",
    "        for _ in range(jitter_per_record):\n",
    "            jittered_lat, jittered_lon = jitter_point(row[lat_col], row[lon_col], max_distance_km)\n",
    "            \n",
    "            # Create a copy of the original record and update its latitude and longitude with the jittered values.\n",
    "            new_record = row.copy()\n",
    "            new_record[lat_col] = jittered_lat\n",
    "            new_record[lon_col] = jittered_lon\n",
    "            \n",
    "            # Add the jittered record to the list.\n",
    "            jittered_records.append(new_record)\n",
    "    \n",
    "    # Create a DataFrame from the jittered records.\n",
    "    jittered_df = pd.DataFrame(jittered_records)\n",
    "    # Combine the original and jittered records into a single DataFrame.\n",
    "    combined_df = pd.concat([df, jittered_df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Specify the number of jittered records to create per original record and the maximum jitter distance (km).\n",
    "jitter_per_record = 10  \n",
    "max_jitter_distance_km = 5  \n",
    "\n",
    "# Generate the expanded dataset by combining the original data with its jittered versions.\n",
    "expanded_df = generate_multiple_jittered_records(\n",
    "    df, \n",
    "    lat_col='latitude', \n",
    "    lon_col='longitude', \n",
    "    jitter_per_record=jitter_per_record, \n",
    "    max_distance_km=max_jitter_distance_km\n",
    ")\n",
    "\n",
    "# Save the expanded dataset with jittered records to a CSV file.\n",
    "expanded_df.to_csv('wild_yak_expanded_10x_jitter.csv', index=False)\n",
    "\n",
    "# Output the first 15 rows of the expanded dataset and display the number of records before and after expansion.\n",
    "print(expanded_df.head(15))\n",
    "print(f\"Original count: {len(df)}, Expanded count: {len(expanded_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211bf93-8cdc-4080-8a2c-22cb8165b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import geodatasets\n",
    "\n",
    "# Load the cleaned dataset containing the occurrence records.\n",
    "df = pd.read_csv(\"wild_yak_Final_cleaned.csv\")\n",
    "\n",
    "# Create a list of shapely Point objects from the longitude and latitude columns.\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "# Convert the DataFrame to a GeoDataFrame with the specified coordinate reference system (WGS84).\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Load a simple world map from geodatasets for the background.\n",
    "world = gpd.read_file(geodatasets.get_path(\"naturalearth.land\"))\n",
    "\n",
    "# Create a sorted list of unique years found in the dataset.\n",
    "years = sorted(gdf['year'].dropna().unique())\n",
    "\n",
    "# Generate a map for each individual year.\n",
    "for year in years:\n",
    "    # Filter the GeoDataFrame to include only the records for the current year.\n",
    "    yearly_data = gdf[gdf['year'] == year]\n",
    "\n",
    "    # Create a new plot figure with a defined size.\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # Plot the world map as the base layer.\n",
    "    world.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "    # Plot the wild yak occurrence points for the current year on top of the world map.\n",
    "    yearly_data.plot(ax=ax, color='red', markersize=20, alpha=0.6)\n",
    "\n",
    "    # Set the title and axis labels for the plot.\n",
    "    plt.title(f\"Wild Yak Occurrences - {year}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    # Save the resulting plot to a PNG file named for the current year.\n",
    "    plt.savefig(f\"wild_yak_{year}.png\")\n",
    "    # Display the plot.\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b889dc-a2ef-434c-8ece-c8b52c51ff0a",
   "metadata": {},
   "source": [
    "## 2.2) Modeling and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8d96a-67b4-482a-b8ea-56c17f78619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Define file paths and directories for input and output data.\n",
    "occurrence_csv = \"wild_yak_Final_cleaned.csv\"   # CSV containing occurrence records.\n",
    "elevation_path = \"elevation_resampled_to_climate.tif\"  # Resampled elevation raster file.\n",
    "landmask_path = \"landmask_asia.tif\"               # Landmask raster file for Asia.\n",
    "climate_root = r\"C:\\Users\\FENIL\\Downloads\\climate_data\"  # Root directory for climate data.\n",
    "ppt_dir = os.path.join(climate_root, \"ppt_1990_2020\")\n",
    "tmin_dir = os.path.join(climate_root, \"tmin_1990_2020\")\n",
    "tmax_dir = os.path.join(climate_root, \"tmax_1990_2020\")\n",
    "output_dir = \"sdm_final\"                         # Directory to save final outputs.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the years for which to run the analysis and the future climate scenarios.\n",
    "selected_years = list(range(2009, 2025))\n",
    "future_scenarios = {2050: {\"SSP585\": \"2050585\", \"SSP245\": \"2050245\"}}\n",
    "\n",
    "# Set a random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========== LOAD OCCURRENCE DATA ==========\n",
    "print(\"Loading occurrence records...\")\n",
    "# Read in the occurrence data and keep only records with valid latitude, longitude, and year values.\n",
    "occurrences = pd.read_csv(occurrence_csv)\n",
    "occurrences = occurrences[['latitude', 'longitude', 'year']].dropna()\n",
    "# Filter the occurrences to include only those in the selected years.\n",
    "occurrences = occurrences[occurrences['year'].isin(selected_years)]\n",
    "\n",
    "# ========== LOAD RASTER DATA ==========\n",
    "print(\"Opening elevation and landmask rasters...\")\n",
    "# Open the resampled elevation raster and read the first band.\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "    transform = elev_src.transform  # Get the spatial transform for coordinate conversion.\n",
    "\n",
    "# Open the landmask raster, converting the data to boolean values.\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1).astype(bool)\n",
    "\n",
    "# ========== FEATURE COLLECTION ==========\n",
    "# Initialize lists to hold features and corresponding presence/absence labels.\n",
    "X_all, y_all = [], []\n",
    "\n",
    "# Loop through each selected year to extract climate and elevation features.\n",
    "for year in selected_years:\n",
    "    print(f\"Processing year {year}...\")\n",
    "    # Load climate data for precipitation, minimum temperature, and maximum temperature.\n",
    "    ppt = xr.open_dataset(os.path.join(ppt_dir, f\"TerraClimate_ppt_{year}.nc\"))['ppt'].sum(dim='time')\n",
    "    tmin = xr.open_dataset(os.path.join(tmin_dir, f\"TerraClimate_tmin_{year}.nc\"))['tmin'].mean(dim='time')\n",
    "    tmax = xr.open_dataset(os.path.join(tmax_dir, f\"TerraClimate_tmax_{year}.nc\"))['tmax'].mean(dim='time')\n",
    "\n",
    "    # Filter occurrence records for the current year (presence data).\n",
    "    presences = occurrences[occurrences['year'] == year]\n",
    "\n",
    "    # For each presence point, extract the climate variables and elevation.\n",
    "    for _, row in presences.iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        # Convert geographic coordinates to raster row and column indices.\n",
    "        i, j = rowcol(transform, lon, lat)\n",
    "        # Skip the record if the point does not fall on land.\n",
    "        if not landmask[i, j]:\n",
    "            continue\n",
    "        feat = [\n",
    "            float(ppt.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            float(tmin.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            float(tmax.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            elev[i, j]\n",
    "        ]\n",
    "        # Only include the record if all features are valid numbers.\n",
    "        if not np.isnan(feat).any():\n",
    "            X_all.append(feat)\n",
    "            y_all.append(1)\n",
    "\n",
    "    # Generate absence points by randomly sampling locations and extracting features.\n",
    "    absences, attempts = 0, 0\n",
    "    lats, lons = ppt.lat.values, ppt.lon.values\n",
    "    # Try to generate twice as many absence points as there are presence points.\n",
    "    while absences < len(presences) * 2 and attempts < 10000:\n",
    "        rand_lat, rand_lon = np.random.choice(lats), np.random.choice(lons)\n",
    "        attempts += 1\n",
    "        i, j = rowcol(transform, rand_lon, rand_lat)\n",
    "        if not landmask[i, j]:\n",
    "            continue\n",
    "        feat = [\n",
    "            float(ppt.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            float(tmin.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            float(tmax.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            elev[i, j]\n",
    "        ]\n",
    "        if not np.isnan(feat).any():\n",
    "            X_all.append(feat)\n",
    "            y_all.append(0)\n",
    "            absences += 1\n",
    "\n",
    "print(\"Features collected.\")\n",
    "\n",
    "# ========== TRAIN MODEL ==========\n",
    "print(\"Training model...\")\n",
    "# Convert the feature and label lists into NumPy arrays.\n",
    "X, y = np.array(X_all), np.array(y_all)\n",
    "# Split the data into training and testing subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Initialize and train a Random Forest classifier.\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluate model performance using the Area Under the ROC Curve (AUC).\n",
    "auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(f\"Model trained. AUC: {auc:.3f}\")\n",
    "\n",
    "# Save the trained model to a file using pickle.\n",
    "model_filename = os.path.join(output_dir, \"random_forest_model.pkl\")\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# ========== PREDICTION AND PLOTTING ==========\n",
    "# The prediction and plotting functions should be included here.\n",
    "# Replace the following placeholder with your actual 'predict_and_plot' function.\n",
    "def predict_and_plot(year, suffix, label):\n",
    "    # Placeholder function: implement your prediction and plotting logic here.\n",
    "    # Typically, this function should load the necessary climate data for the given year,\n",
    "    # use the trained model to predict species distribution,\n",
    "    # and generate maps that visualize the predictions.\n",
    "    print(f\"Running predictions and plotting for year {year} with label {label}.\")\n",
    "\n",
    "# ========== RUN PREDICTIONS ==========\n",
    "# Run predictions and plot results for each selected year.\n",
    "for year in selected_years:\n",
    "    predict_and_plot(year, year, str(year))\n",
    "\n",
    "# Run predictions for future climate scenarios.\n",
    "for future_year, scenarios in future_scenarios.items():\n",
    "    for label, suffix in scenarios.items():\n",
    "        predict_and_plot(future_year, suffix, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4988e7-d6cf-457a-8340-d6138ba4b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# === Configuration ===\n",
    "# Specify paths for input occurrence data, elevation raster, and landmask.\n",
    "occurrence_csv = \"wild_yak_Final_cleaned.csv\"\n",
    "elevation_path = \"elevation_resampled_to_climate.tif\"\n",
    "landmask_path = \"landmask_asia.tif\"\n",
    "\n",
    "# Define the root directory for climate data and subdirectories for different variables.\n",
    "climate_root = r\"C:\\Users\\FENIL\\Downloads\\climate_data\"\n",
    "ppt_dir = os.path.join(climate_root, \"ppt_1990_2020\")\n",
    "tmin_dir = os.path.join(climate_root, \"tmin_1990_2020\")\n",
    "tmax_dir = os.path.join(climate_root, \"tmax_1990_2020\")\n",
    "\n",
    "# Define the range of years to use for model training and evaluation.\n",
    "selected_years = list(range(2009, 2025))\n",
    "\n",
    "# Set a fixed random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# === Load Occurrence Data ===\n",
    "print(\"Loading occurrence data...\")\n",
    "# Read occurrence records and select only the necessary columns (latitude, longitude, year).\n",
    "occurrences = pd.read_csv(occurrence_csv)\n",
    "occurrences = occurrences[['latitude', 'longitude', 'year']].dropna()\n",
    "# Filter records to keep only those that fall within the selected years.\n",
    "occurrences = occurrences[occurrences['year'].isin(selected_years)]\n",
    "\n",
    "# === Load Elevation and Landmask ===\n",
    "print(\"Loading elevation and landmask data...\")\n",
    "# Open the elevation raster and extract the first band along with its spatial transform.\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "    transform = elev_src.transform\n",
    "\n",
    "# Open the landmask raster and convert its values to boolean for easier processing.\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1).astype(bool)\n",
    "\n",
    "# === Feature Collection ===\n",
    "print(\"Building features...\")\n",
    "# Initialize lists to collect features (X_all) and labels (y_all).\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for year in selected_years:\n",
    "    print(f\"  Processing year {year}\")\n",
    "    # Load annual climate data:\n",
    "    # - Sum precipitation over the year.\n",
    "    # - Compute mean minimum and maximum temperatures.\n",
    "    ppt = xr.open_dataset(os.path.join(ppt_dir, f\"TerraClimate_ppt_{year}.nc\"))['ppt'].sum(dim='time')\n",
    "    tmin = xr.open_dataset(os.path.join(tmin_dir, f\"TerraClimate_tmin_{year}.nc\"))['tmin'].mean(dim='time')\n",
    "    tmax = xr.open_dataset(os.path.join(tmax_dir, f\"TerraClimate_tmax_{year}.nc\"))['tmax'].mean(dim='time')\n",
    "\n",
    "    # Get available latitude and longitude arrays from the precipitation dataset.\n",
    "    lats = ppt.lat.values\n",
    "    lons = ppt.lon.values\n",
    "\n",
    "    # Extract occurrence records (presences) for the current year.\n",
    "    presences = occurrences[occurrences['year'] == year]\n",
    "\n",
    "    # Process each occurrence record.\n",
    "    for _, row in presences.iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        try:\n",
    "            # Convert geographic coordinates to raster indices.\n",
    "            i, j = rowcol(transform, lon, lat)\n",
    "            # Skip this record if it does not fall on land.\n",
    "            if not landmask[i, j]:\n",
    "                continue\n",
    "            elev_val = elev[i, j]\n",
    "            # Extract climate features using nearest-neighbor selection.\n",
    "            feat = [\n",
    "                float(ppt.sel(lat=lat, lon=lon, method='nearest')),\n",
    "                float(tmin.sel(lat=lat, lon=lon, method='nearest')),\n",
    "                float(tmax.sel(lat=lat, lon=lon, method='nearest')),\n",
    "                elev_val\n",
    "            ]\n",
    "            # Append the features and label (1 for presence) if all values are valid.\n",
    "            if not any(np.isnan(feat)):\n",
    "                X_all.append(feat)\n",
    "                y_all.append(1)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Generate pseudo-absence points randomly.\n",
    "    absences = 0\n",
    "    while absences < len(presences) * 2:\n",
    "        # Randomly select a latitude and longitude from the available climate grid.\n",
    "        rand_lat = float(np.random.choice(lats))\n",
    "        rand_lon = float(np.random.choice(lons))\n",
    "        try:\n",
    "            i, j = rowcol(transform, rand_lon, rand_lat)\n",
    "            # Ensure the randomly chosen point falls on land.\n",
    "            if not landmask[i, j]:\n",
    "                continue\n",
    "            elev_val = elev[i, j]\n",
    "            feat = [\n",
    "                float(ppt.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "                float(tmin.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "                float(tmax.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "                elev_val\n",
    "            ]\n",
    "            # Add the pseudo-absence record if it contains valid feature values.\n",
    "            if not any(np.isnan(feat)):\n",
    "                X_all.append(feat)\n",
    "                y_all.append(0)\n",
    "                absences += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "print(f\"\\nFeatures collected: {len(X_all)} samples\")\n",
    "\n",
    "# === Model Evaluation ===\n",
    "# Convert the feature and label lists to NumPy arrays.\n",
    "X = np.array(X_all)\n",
    "y = np.array(y_all)\n",
    "\n",
    "# Split the dataset into training and testing subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Load the pre-trained Random Forest model.\n",
    "with open(\"sdm_final/random_forest_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Make predictions on the test set.\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the ROC AUC score to evaluate model performance.\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "print(f\"\\nROC AUC Score: {auc:.3f}\")\n",
    "\n",
    "# Display the confusion matrix.\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Display the full classification report.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the ROC Curve using the estimator's performance on the test data.\n",
    "RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Feature Importance as a horizontal bar chart.\n",
    "feature_names = ['Precipitation', 'Min Temp', 'Max Temp', 'Elevation']\n",
    "importances = model.feature_importances_\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(feature_names, importances, color='orange')\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print out the feature importances.\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, score in zip(feature_names, importances):\n",
    "    print(f\"  - {name}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63855453-4326-478d-b8d4-de9b3074c320",
   "metadata": {},
   "source": [
    "## 2.3) Prediction and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcac118-1983-40d2-b658-6295afa77cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.ndimage import zoom, gaussian_filter\n",
    "\n",
    "# Define file and directory paths for outputs, elevation, landmask, and climate data.\n",
    "output_dir = \"sdm_final\"\n",
    "elevation_path = \"elevation_resampled_to_climate.tif\"\n",
    "landmask_path = \"landmask_asia.tif\"\n",
    "climate_root = r\"C:\\Users\\FENIL\\Downloads\\climate_data\"\n",
    "ppt_dir = os.path.join(climate_root, \"ppt_1990_2020\")\n",
    "tmin_dir = os.path.join(climate_root, \"tmin_1990_2020\")\n",
    "tmax_dir = os.path.join(climate_root, \"tmax_1990_2020\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the pre-trained Random Forest model from disk.\n",
    "with open(os.path.join(output_dir, \"random_forest_model.pkl\"), 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Open the elevation raster and read the first band.\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "\n",
    "# Open the landmask raster and convert the data to boolean values.\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1).astype(bool)\n",
    "\n",
    "# Function to load monthly climate bands and reduce them either by summing or by averaging.\n",
    "def load_bands(path, reduce='sum'):\n",
    "    ds = xr.open_dataset(path)\n",
    "    bands = [ds[f'Band{i}'] for i in range(1, 13)]\n",
    "    return sum(bands) if reduce == 'sum' else sum(bands) / 12\n",
    "\n",
    "# Function to predict habitat suitability and generate a visualization map.\n",
    "def predict_and_plot(year, suffix, label):\n",
    "    print(f\"\\nProcessing predictions for year {year} ({label})\")\n",
    "\n",
    "    # Determine if the prediction is for a future scenario based on the suffix.\n",
    "    is_future = isinstance(suffix, str) and suffix.startswith(\"2050\")\n",
    "    if is_future:\n",
    "        # For future scenarios, load and process the climate data and reverse the vertical axis.\n",
    "        ppt = load_bands(os.path.join(ppt_dir, f\"TerraClimate_ppt_{suffix}.nc\"), 'sum')[::-1, :]\n",
    "        tmin = load_bands(os.path.join(tmin_dir, f\"TerraClimate_tmin_{suffix}.nc\"), 'mean')[::-1, :]\n",
    "        tmax = load_bands(os.path.join(tmax_dir, f\"TerraClimate_tmax_{suffix}.nc\"), 'mean')[::-1, :]\n",
    "    else:\n",
    "        # For observed data, load the NetCDF file and compute the necessary aggregate values.\n",
    "        ppt = xr.open_dataset(os.path.join(ppt_dir, f\"TerraClimate_ppt_{year}.nc\"))['ppt'].sum(dim='time')\n",
    "        tmin = xr.open_dataset(os.path.join(tmin_dir, f\"TerraClimate_tmin_{year}.nc\"))['tmin'].mean(dim='time')\n",
    "        tmax = xr.open_dataset(os.path.join(tmax_dir, f\"TerraClimate_tmax_{year}.nc\"))['tmax'].mean(dim='time')\n",
    "\n",
    "    # Stack the climate and elevation variables into a 2D feature array.\n",
    "    flat_features = np.stack([\n",
    "        ppt.values.flatten(),\n",
    "        tmin.values.flatten(),\n",
    "        tmax.values.flatten(),\n",
    "        elev.flatten()\n",
    "    ], axis=1)\n",
    "\n",
    "    # Create a mask for valid data locations (non-NaN values and on land).\n",
    "    valid = (~np.isnan(flat_features).any(axis=1)) & landmask.flatten()\n",
    "\n",
    "    # Prepare a full array to hold the prediction probabilities.\n",
    "    y_pred = np.full(ppt.values.size, np.nan)\n",
    "    # For valid data points, compute the probability of presence using the trained model.\n",
    "    y_pred[valid] = model.predict_proba(flat_features[valid])[:, 1]\n",
    "\n",
    "    # Reshape the predicted probabilities to match the original climate raster shape.\n",
    "    suitability_map = y_pred.reshape(ppt.values.shape)\n",
    "    # Assign NaN to locations not on land.\n",
    "    suitability_map[~landmask] = np.nan\n",
    "\n",
    "    # Save the raw suitability map as a NumPy binary file.\n",
    "    np.save(os.path.join(output_dir, f\"suitability_map_{year}_{label}.npy\"), suitability_map)\n",
    "\n",
    "    # Prepare a display map by filtering out low suitability values.\n",
    "    display_map = suitability_map.copy()\n",
    "    display_map[display_map <= 0.5] = np.nan\n",
    "    # Increase the resolution of the map for better visualization.\n",
    "    upscale = zoom(display_map, 3, order=1)\n",
    "    # Apply a Gaussian filter to smooth the map.\n",
    "    smooth_map = gaussian_filter(upscale, sigma=1.2)\n",
    "\n",
    "    # Create a figure with the Plate Carree projection.\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    # Set the spatial extent for the map display.\n",
    "    ax.set_extent([40, 140, -5, 60], crs=ccrs.PlateCarree())\n",
    "    # Add a background image and geographic features.\n",
    "    ax.stock_img()\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.4)\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.4)\n",
    "\n",
    "    # Display the smoothed habitat suitability map.\n",
    "    img = ax.imshow(smooth_map, cmap='YlOrRd', alpha=0.4,\n",
    "                    extent=[float(ppt.lon.min()), float(ppt.lon.max()), float(ppt.lat.min()), float(ppt.lat.max())],\n",
    "                    transform=ccrs.PlateCarree(), vmin=0.5, vmax=1)\n",
    "\n",
    "    # Add a horizontal colorbar with an appropriate label.\n",
    "    cbar = plt.colorbar(img, orientation='horizontal', pad=0.05, shrink=0.75)\n",
    "    cbar.set_label(f\"Wild Yak Suitability ({label})\")\n",
    "    plt.title(f\"Wild Yak Habitat Suitability - {year}\", fontsize=14)\n",
    "\n",
    "    # Add an arrow and label to indicate the North direction.\n",
    "    ax.annotate('', xy=(0.94, 0.88), xytext=(0.94, 0.82),\n",
    "                xycoords='axes fraction', arrowprops=dict(facecolor='black', arrowstyle='-|>', lw=1.5))\n",
    "    ax.text(0.94, 0.89, 'N', transform=ax.transAxes,\n",
    "            horizontalalignment='center', verticalalignment='bottom',\n",
    "            fontsize=12, fontweight='bold', color='black')\n",
    "\n",
    "    # Annotate country names on the map for reference.\n",
    "    country_labels = {\n",
    "        'India': ((78, 22), 9), 'China': ((104, 35), 10), 'Nepal': ((84, 28.5), 8),\n",
    "        'Mongolia': ((103, 47), 9), 'Bangladesh': ((90, 24), 8), 'Pakistan': ((70, 30), 8),\n",
    "        'Afghanistan': ((66, 34), 8), 'Bhutan': ((91.5, 27.5), 7),\n",
    "        'Myanmar': ((96, 21), 8), 'Kazakhstan': ((70, 48), 9)\n",
    "    }\n",
    "    for name, ((lon, lat), fontsize) in country_labels.items():\n",
    "        ax.text(lon, lat, name, transform=ccrs.PlateCarree(),\n",
    "                fontsize=fontsize, fontweight='bold', ha='center', color='black',\n",
    "                bbox=dict(facecolor='white', alpha=0.6, boxstyle='round,pad=0.2'))\n",
    "\n",
    "    # Save the generated map as a high-resolution PNG file.\n",
    "    save_path = os.path.join(output_dir, f\"yak_suitability_{year}_{label}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Map and .npy file saved: {save_path}\")\n",
    "\n",
    "# Define years for which predictions will be generated.\n",
    "selected_years = list(range(2009, 2025))\n",
    "# Define future climate scenarios for the year 2050 with different SSP labels.\n",
    "future_scenarios = {2050: {\"SSP585\": \"2050585\", \"SSP245\": \"2050245\"}}\n",
    "\n",
    "# Generate predictions and plot maps for each selected observed year.\n",
    "for year in selected_years:\n",
    "    predict_and_plot(year, year, str(year))\n",
    "\n",
    "# Generate predictions and plot maps for each future climate scenario.\n",
    "for future_year, scenarios in future_scenarios.items():\n",
    "    for label, suffix in scenarios.items():\n",
    "        predict_and_plot(future_year, suffix, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8c9da-3fd1-48e0-87ff-e4481773e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "# Path to the input landmask raster file.\n",
    "tif_path = \"landmask_asia.tif\"\n",
    "# Path where the NumPy binary file (npy) will be saved.\n",
    "output_path = \"sdm_final/landmask.npy\"\n",
    "\n",
    "# Open the raster file and read the first band.\n",
    "with rasterio.open(tif_path) as src:\n",
    "    land_data = src.read(1)  # Read the first band of the raster.\n",
    "    # Convert the raster data to a boolean mask where values greater than 0 indicate land.\n",
    "    landmask = (land_data > 0)\n",
    "\n",
    "# Save the boolean landmask array as a NumPy .npy file.\n",
    "np.save(output_path, landmask)\n",
    "print(f\"Landmask saved to: {output_path}\")\n",
    "print(f\"Shape: {landmask.shape}, Land pixels: {np.count_nonzero(landmask)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4280462-6861-4fe2-a2a0-aa6d44df892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "output_dir = \"sdm_final\"\n",
    "land_raster_path = os.path.join(\"landmask_asia.tif\")\n",
    "landmask_npy_path = os.path.join(output_dir, \"landmask_asia_cropped.npy\")\n",
    "years = list(range(2009, 2025))\n",
    "future = {\n",
    "    \"2050_SSP245\": \"suitability_map_2050_SSP245.npy\",\n",
    "    \"2050_SSP585\": \"suitability_map_2050_SSP585.npy\"\n",
    "}\n",
    "threshold = 0.5\n",
    "pixel_area_km2 = 13.67  # Area per pixel (2.5 arcmin resolution)\n",
    "\n",
    "# --- STEP 1: Load landmask raster and manually crop to Asia region ---\n",
    "with rasterio.open(land_raster_path) as src:\n",
    "    land_data = src.read(1)  # Read first band\n",
    "\n",
    "# Manual crop to approximate Asia region (based on your raster grid: 4320 x 8640)\n",
    "# Adjust these indices if needed — this focuses on central & eastern Asia\n",
    "asia_mask = np.zeros_like(land_data, dtype=bool)\n",
    "asia_mask[1000:3500, 2000:7000] = land_data[1000:3500, 2000:7000] > 0\n",
    "\n",
    "# Save the cropped Asia landmask\n",
    "np.save(landmask_npy_path, asia_mask)\n",
    "print(f\"✅ Saved cropped Asia landmask: {landmask_npy_path}\")\n",
    "print(f\"Pixels in Asia mask: {np.count_nonzero(asia_mask)}\")\n",
    "print(f\"Estimated total Asia area: {np.count_nonzero(asia_mask) * pixel_area_km2:,.2f} km²\")\n",
    "\n",
    "# --- STEP 2: Function to calculate suitable area in km² using Asia mask ---\n",
    "def count_suitable_area_km2(npy_path, threshold, mask):\n",
    "    data = np.load(npy_path)\n",
    "    masked_data = np.where(mask, data, np.nan)\n",
    "    suitable_pixels = np.count_nonzero(masked_data > threshold)\n",
    "    return suitable_pixels * pixel_area_km2\n",
    "\n",
    "# --- STEP 3: Collect suitable area for each year ---\n",
    "area_by_year = []\n",
    "for year in years:\n",
    "    npy_file = os.path.join(output_dir, f\"suitability_map_{year}_{year}.npy\")\n",
    "    if os.path.exists(npy_file):\n",
    "        area = count_suitable_area_km2(npy_file, threshold, asia_mask)\n",
    "        area_by_year.append((year, area))\n",
    "    else:\n",
    "        print(f\"⚠️ Missing file: {npy_file}\")\n",
    "\n",
    "# --- STEP 4: Add future climate scenarios ---\n",
    "for label, fname in future.items():\n",
    "    npy_path = os.path.join(output_dir, fname)\n",
    "    if os.path.exists(npy_path):\n",
    "        area = count_suitable_area_km2(npy_path, threshold, asia_mask)\n",
    "        area_by_year.append((label, area))\n",
    "    else:\n",
    "        print(f\"⚠️ Missing file: {npy_path}\")\n",
    "\n",
    "# --- STEP 5: Plot the trend ---\n",
    "area_by_year = sorted(area_by_year, key=lambda x: str(x[0]))\n",
    "labels, areas_km2 = zip(*area_by_year)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(labels, areas_km2, marker='o', linewidth=2, color='seagreen', label='Suitable Area')\n",
    "\n",
    "if '2050_SSP245' in labels:\n",
    "    plt.axvline('2050_SSP245', color='orange', linestyle='--', label='SSP245 (2050)')\n",
    "if '2050_SSP585' in labels:\n",
    "    plt.axvline('2050_SSP585', color='red', linestyle='--', label='SSP585 (2050)')\n",
    "\n",
    "plt.title(f\"Suitable Habitat Area for Wild Yaks in Asia (Threshold > {threshold})\", fontsize=14)\n",
    "plt.ylabel(\"Suitable Area (km²)\", fontsize=12)\n",
    "plt.xlabel(\"Year / Scenario\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = os.path.join(output_dir, \"yak_suitability_area_trend_final.png\")\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"📊 Plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb99a2-61a0-49b2-a076-49c205f173ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the CSV file containing centroid shifts.\n",
    "csv_path = os.path.join(\"sdm_final\", \"centroid_shifts.csv\")\n",
    "# Define the output directory and file name for the trend plot.\n",
    "output_dir = \"sdm_final\"\n",
    "output_file = os.path.join(output_dir, \"centroid_shift_trends.png\")\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "# Read the CSV file into a DataFrame.\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a numeric year field by extracting the numeric part from the 'Year' column.\n",
    "# This handles entries with an underscore by splitting and converting the first part to an integer.\n",
    "df[\"Year_Num\"] = df[\"Year\"].apply(lambda x: int(x.split(\"_\")[0]) if \"_\" in x else int(x))\n",
    "# Sort the DataFrame based on the numeric year.\n",
    "df = df.sort_values(\"Year_Num\")\n",
    "\n",
    "# Create a mask to identify future scenarios (where the 'Year' string contains \"2050\").\n",
    "future_mask = df[\"Year\"].str.contains(\"2050\", na=False)\n",
    "\n",
    "# --- Plotting ---\n",
    "# Create a figure with three vertically stacked subplots sharing the x-axis.\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot the centroid latitude trend.\n",
    "axs[0].plot(df[\"Year\"], df[\"Centroid_Lat\"], marker='o', color='royalblue')\n",
    "# Highlight future scenarios with a scatter plot.\n",
    "axs[0].scatter(df[\"Year\"][future_mask], df[\"Centroid_Lat\"][future_mask], color='black', label='2050 Scenarios')\n",
    "axs[0].set_ylabel(\"Latitude (°N)\")\n",
    "axs[0].set_title(\"Centroid Latitude Shift\")\n",
    "axs[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot the centroid longitude trend.\n",
    "axs[1].plot(df[\"Year\"], df[\"Centroid_Lon\"], marker='o', color='forestgreen')\n",
    "axs[1].scatter(df[\"Year\"][future_mask], df[\"Centroid_Lon\"][future_mask], color='black')\n",
    "axs[1].set_ylabel(\"Longitude (°E)\")\n",
    "axs[1].set_title(\"Centroid Longitude Shift\")\n",
    "axs[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot the median elevation trend.\n",
    "axs[2].plot(df[\"Year\"], df[\"Median_Elevation\"], marker='o', color='firebrick')\n",
    "axs[2].scatter(df[\"Year\"][future_mask], df[\"Median_Elevation\"][future_mask], color='black')\n",
    "axs[2].set_ylabel(\"Elevation (m)\")\n",
    "axs[2].set_title(\"Median Elevation Shift\")\n",
    "axs[2].grid(True, linestyle='--', alpha=0.5)\n",
    "axs[2].set_xlabel(\"Year / Scenario\")\n",
    "\n",
    "# Rotate the x-axis labels for better readability.\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a high-resolution PNG file.\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Centroid trend plot saved: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814584ef-5832-4f70-b351-2893c0bb290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-imports after kernel reset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load centroid data\n",
    "df = pd.read_csv(\"sdm_final/centroid_shifts.csv\")\n",
    "df[\"Year_Num\"] = df[\"Year\"].apply(lambda x: int(x.split(\"_\")[0]) if \"_\" in x else int(x))\n",
    "df = df.sort_values(\"Year_Num\").reset_index(drop=True)\n",
    "\n",
    "# Calculate distance shifts between consecutive years\n",
    "distances_km = [0]  # Start with 0 for the first year\n",
    "for i in range(1, len(df)):\n",
    "    prev = (df.loc[i - 1, \"Centroid_Lat\"], df.loc[i - 1, \"Centroid_Lon\"])\n",
    "    curr = (df.loc[i, \"Centroid_Lat\"], df.loc[i, \"Centroid_Lon\"])\n",
    "    distance = geodesic(prev, curr).kilometers\n",
    "    distances_km.append(distance)\n",
    "\n",
    "df[\"Distance_Change_km\"] = distances_km\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(\"sdm_final\", \"centroid_distance_changes.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873f8c3-1168-40b1-ab44-6da9211bbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load the centroid data from the CSV file.\n",
    "df = pd.read_csv(\"sdm_final/centroid_shifts.csv\")\n",
    "\n",
    "# Compute distances between successive centroids.\n",
    "# Start with 0 km for the first year as there is no previous location.\n",
    "distances = [0.0]\n",
    "for i in range(1, len(df)):\n",
    "    # Get the latitude and longitude of the previous centroid.\n",
    "    prev = (df.loc[i-1, \"Centroid_Lat\"], df.loc[i-1, \"Centroid_Lon\"])\n",
    "    # Get the latitude and longitude of the current centroid.\n",
    "    curr = (df.loc[i, \"Centroid_Lat\"], df.loc[i, \"Centroid_Lon\"])\n",
    "    # Calculate the geodesic distance in kilometers between the two centroids.\n",
    "    dist_km = geodesic(prev, curr).kilometers\n",
    "    # Append the distance (rounded to 2 decimal places) to the distances list.\n",
    "    distances.append(round(dist_km, 2))\n",
    "\n",
    "# Set up a colormap to assign a unique color for each year in the dataset.\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(df))\n",
    "colors = [cmap(i) for i in range(len(df))]\n",
    "# Create labels for each point that include the year and the computed distance.\n",
    "labels = [f\"{year} ({dist} km)\" for year, dist in zip(df[\"Year\"], distances)]\n",
    "\n",
    "# Create a scatter plot with arrows connecting centroids.\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(df)):\n",
    "    # Plot the centroid as a scatter point.\n",
    "    plt.scatter(df.loc[i, \"Centroid_Lon\"], df.loc[i, \"Centroid_Lat\"], color=colors[i], s=60, label=labels[i])\n",
    "    # If there is a previous point, draw a line connecting the previous and current centroids.\n",
    "    if i > 0:\n",
    "        plt.plot([df.loc[i-1, \"Centroid_Lon\"], df.loc[i, \"Centroid_Lon\"]],\n",
    "                 [df.loc[i-1, \"Centroid_Lat\"], df.loc[i, \"Centroid_Lat\"]],\n",
    "                 color=\"black\", linewidth=1)\n",
    "\n",
    "# Set the title and labels for the plot.\n",
    "plt.title(\"Wild Yak Habitat Centroid Shift with Distance (2009–2024 + 2050)\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "# Add a legend that displays the year and the distance, placing it outside the plot area.\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), title=\"Year (Distance)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the final plot as a high-resolution PNG file.\n",
    "final_path = os.path.join(\"sdm_final\", \"centroid_arrow_clean_map.png\")\n",
    "plt.savefig(final_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094023d-4daa-454b-baea-8119fa886f64",
   "metadata": {},
   "source": [
    "# 3) SDM for Takin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8dd9f-9903-45c7-b578-38a15ff2c76b",
   "metadata": {},
   "source": [
    "## 3.1) Data cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ecc31-0ab2-426a-a407-e51656856c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === Step 1: Load your CSV ===\n",
    "input_file = \"C:\\\\Users\\\\FENIL\\\\Downloads\\\\budorcas_taxicolor_occurrences.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# === Step 2: Extract year from date ===\n",
    "def extract_year(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    if '/' in date_str:\n",
    "        date_str = date_str.split('/')[0]\n",
    "    match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None\n",
    "\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "\n",
    "# === Step 3: Filter by year (only >= 2009) ===\n",
    "df = df[df['year'] >= 2009]\n",
    "\n",
    "# === Step 4: Filter by region (remove Europe and Americas) ===\n",
    "# General bounds for Asia:\n",
    "# - Latitude: 5 to 60\n",
    "# - Longitude: 60 to 150\n",
    "df = df[\n",
    "    (df['latitude'] >= 5) & (df['latitude'] <= 60) &\n",
    "    (df['longitude'] >= 60) & (df['longitude'] <= 150)\n",
    "]\n",
    "\n",
    "# Optional: Narrow further to Himalayan-Tibetan region\n",
    "# Example bounds (customize if needed):\n",
    "# Latitude: 25 to 40\n",
    "# Longitude: 75 to 100\n",
    "# df = df[\n",
    "#     (df['latitude'] >= 25) & (df['latitude'] <= 40) &\n",
    "#     (df['longitude'] >= 75) & (df['longitude'] <= 100)\n",
    "# ]\n",
    "\n",
    "# === Step 5: Drop the original date column (if needed) ===\n",
    "df_cleaned = df.drop(columns=['date'])\n",
    "\n",
    "# === Step 6: Save cleaned data ===\n",
    "output_file = \"takin_cleaned_asia_after2009.csv\"\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b66fcd-b929-4efa-a5b2-e81ff8fb8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import distance\n",
    "from geopy import Point\n",
    "import random\n",
    "\n",
    "# === Step 1: Load the cleaned CSV ===\n",
    "df = pd.read_csv(\"takin_cleaned_asia_after2009.csv\")\n",
    "\n",
    "# === Step 2: Function to generate a random point within 5 km ===\n",
    "def jitter_point(lat, lon, max_km=5):\n",
    "    # Random bearing (0 to 360°)\n",
    "    bearing = random.uniform(0, 360)\n",
    "    # Random distance (0 to max_km)\n",
    "    dist = random.uniform(0, max_km)\n",
    "    origin = Point(lat, lon)\n",
    "    destination = distance(kilometers=dist).destination(origin, bearing)\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "# === Step 3: Create jittered points ===\n",
    "jittered_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for _ in range(5):  # 3 jittered points per original\n",
    "        jitter_lat, jitter_lon = jitter_point(row['latitude'], row['longitude'])\n",
    "        jittered_data.append({\n",
    "            'scientificName': row['scientificName'],\n",
    "            'latitude': jitter_lat,\n",
    "            'longitude': jitter_lon,\n",
    "            'source': row['source'],\n",
    "            'year': row['year']\n",
    "        })\n",
    "\n",
    "# === Step 4: Combine original and jittered data ===\n",
    "df_jittered = pd.DataFrame(jittered_data)\n",
    "df_combined = pd.concat([df, df_jittered], ignore_index=True)\n",
    "\n",
    "# === Step 5: Save to CSV ===\n",
    "output_file = \"takin_Final_cleaned.csv\"\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Jittered dataset saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69fce4-4f39-4056-a592-f6d832011956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# === Step 1: Load the cleaned CSV ===\n",
    "df = pd.read_csv(\"takin_Final_cleaned.csv\")\n",
    "\n",
    "# === Step 2: Create a world map ===\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()  # Show the entire world\n",
    "\n",
    "# === Step 3: Add geographic features ===\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.4)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "\n",
    "# === Step 4: Plot occurrence points ===\n",
    "ax.scatter(df['longitude'], df['latitude'],\n",
    "           color='red', s=12, alpha=0.7,\n",
    "           transform=ccrs.PlateCarree(),\n",
    "           label='Takin Occurrence')\n",
    "\n",
    "# === Step 5: Display map ===\n",
    "plt.title(\"Global Distribution of Cleaned Takin Occurrence Points (Post-2009)\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947adb9-4506-4979-9e94-304621e2f3d3",
   "metadata": {},
   "source": [
    "## 3.2) Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42030d1-ea20-4c1f-a7e3-e647ea68102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "# ========= CONFIGURATION =========\n",
    "# Define file paths for occurrence data, elevation and landmask rasters, and climate data directories.\n",
    "occurrence_csv = \"takin_Final_cleaned.csv\"\n",
    "elevation_path = \"elevation_resampled_to_climate.tif\"\n",
    "landmask_path = \"landmask_asia.tif\"\n",
    "climate_root = r\"C:\\Users\\FENIL\\Downloads\\climate_data\"\n",
    "ppt_dir = os.path.join(climate_root, \"ppt_1990_2020\")\n",
    "tmin_dir = os.path.join(climate_root, \"tmin_1990_2020\")\n",
    "tmax_dir = os.path.join(climate_root, \"tmax_1990_2020\")\n",
    "output_dir = \"sdm_takin\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the years for processing and future scenario information.\n",
    "selected_years = list(range(2009, 2025))\n",
    "future_scenarios = {2050: {\"SSP585\": \"2050585\", \"SSP245\": \"2050245\"}}\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= LOAD OCCURRENCE DATA =========\n",
    "print(\"Loading occurrence records...\")\n",
    "# Read the occurrence records, keeping only the required columns and dropping missing values.\n",
    "occurrences = pd.read_csv(occurrence_csv)\n",
    "occurrences = occurrences[['latitude', 'longitude', 'year']].dropna()\n",
    "# Filter occurrence records to include only the selected years.\n",
    "occurrences = occurrences[occurrences['year'].isin(selected_years)]\n",
    "\n",
    "# ========= LOAD RASTER DATA =========\n",
    "print(\"Opening elevation and landmask rasters...\")\n",
    "# Open the elevation raster and read the first band along with its spatial transform.\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "    transform = elev_src.transform\n",
    "\n",
    "# Open the landmask raster and convert the data into a boolean array.\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1).astype(bool)\n",
    "\n",
    "# ========= FEATURE COLLECTION =========\n",
    "# Initialize lists to store features and corresponding presence/absence labels.\n",
    "X_all, y_all = [], []\n",
    "for year in selected_years:\n",
    "    print(f\"Processing year {year}...\")\n",
    "    # Load climate data for precipitation (total annual sum),\n",
    "    # and minimum and maximum temperatures (annual means) using xarray.\n",
    "    ppt = xr.open_dataset(os.path.join(ppt_dir, f\"TerraClimate_ppt_{year}.nc\"))['ppt'].sum(dim='time')\n",
    "    tmin = xr.open_dataset(os.path.join(tmin_dir, f\"TerraClimate_tmin_{year}.nc\"))['tmin'].mean(dim='time')\n",
    "    tmax = xr.open_dataset(os.path.join(tmax_dir, f\"TerraClimate_tmax_{year}.nc\"))['tmax'].mean(dim='time')\n",
    "\n",
    "    # Get occurrence records for the current year.\n",
    "    presences = occurrences[occurrences['year'] == year]\n",
    "\n",
    "    # Process each occurrence record (presence points).\n",
    "    for _, row in presences.iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        # Translate latitude and longitude into raster row and column indices.\n",
    "        i, j = rowcol(transform, lon, lat)\n",
    "        # Skip the point if it does not lie on land.\n",
    "        if not landmask[i, j]:\n",
    "            continue\n",
    "        # Extract climate variables at the location and get the corresponding elevation.\n",
    "        feat = [\n",
    "            float(ppt.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            float(tmin.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            float(tmax.sel(lat=lat, lon=lon, method='nearest')),\n",
    "            elev[i, j]\n",
    "        ]\n",
    "        # If all feature values are valid numbers, add the features and assign a label of 1 for presence.\n",
    "        if not np.isnan(feat).any():\n",
    "            X_all.append(feat)\n",
    "            y_all.append(1)\n",
    "\n",
    "    # Generate pseudo-absences by sampling random locations.\n",
    "    absences, attempts = 0, 0\n",
    "    lats, lons = ppt.lat.values, ppt.lon.values\n",
    "    # Continue sampling until twice as many absence points as presences are collected,\n",
    "    # or until a maximum number of attempts is reached.\n",
    "    while absences < len(presences) * 2 and attempts < 10000:\n",
    "        rand_lat, rand_lon = np.random.choice(lats), np.random.choice(lons)\n",
    "        attempts += 1\n",
    "        i, j = rowcol(transform, rand_lon, rand_lat)\n",
    "        # Ensure the random point lies on land.\n",
    "        if not landmask[i, j]:\n",
    "            continue\n",
    "        feat = [\n",
    "            float(ppt.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            float(tmin.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            float(tmax.sel(lat=rand_lat, lon=rand_lon, method='nearest')),\n",
    "            elev[i, j]\n",
    "        ]\n",
    "        # Add the feature if it is valid, and label it as absence (0).\n",
    "        if not np.isnan(feat).any():\n",
    "            X_all.append(feat)\n",
    "            y_all.append(0)\n",
    "            absences += 1\n",
    "\n",
    "print(\"Features collected.\")\n",
    "\n",
    "# ========= TRAIN MODEL =========\n",
    "print(\"Training model...\")\n",
    "# Convert the collected feature list and labels to NumPy arrays.\n",
    "X, y = np.array(X_all), np.array(y_all)\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Initialize a Random Forest classifier with a defined number of trees and train it.\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Compute the AUC score on the test set to evaluate model performance.\n",
    "auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(f\"Model trained. AUC: {auc:.3f}\")\n",
    "\n",
    "# Save the trained model to a file using pickle.\n",
    "model_filename = os.path.join(output_dir, \"random_forest_model_takin.pkl\")\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6f644-388b-467c-99f8-dfa81e9d5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, RocCurveDisplay, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- MODEL EVALUATION ---\n",
    "\n",
    "# Generate model predictions on the test dataset.\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute and print the Area Under the ROC Curve (AUC) score.\n",
    "print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "# --- CONFUSION MATRIX ---\n",
    "# Compute the confusion matrix comparing actual and predicted labels.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "# Draw a heatmap of the confusion matrix using a blue color palette.\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Absence', 'Presence'],\n",
    "            yticklabels=['Absence', 'Presence'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- CLASSIFICATION REPORT ---\n",
    "# Print the detailed classification report including precision, recall, f1-score, etc.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Absence', 'Presence']))\n",
    "\n",
    "# --- ROC CURVE PLOT ---\n",
    "# Plot the ROC Curve to visualize the trade-off between true positive rate and false positive rate.\n",
    "RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n",
    "\n",
    "# --- FEATURE IMPORTANCE ---\n",
    "# Retrieve the feature importance scores and sort them in descending order.\n",
    "feature_names = ['Precipitation', 'Min Temperature', 'Max Temperature', 'Elevation']\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances as a horizontal bar chart.\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=importances[indices], y=[feature_names[i] for i in indices])\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfbc69-f32d-4d18-9237-46447be0ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the output directory where the .npy files are stored.\n",
    "output_dir = \"sdm_takin\"  # Change this if your directory is different.\n",
    "\n",
    "# List all files with the .npy extension in the specified directory.\n",
    "npy_files = [f for f in os.listdir(output_dir) if f.endswith(\".npy\")]\n",
    "if not npy_files:\n",
    "    print(\"No .npy files found in:\", output_dir)\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(npy_files)} .npy files:\")\n",
    "for file in npy_files:\n",
    "    print(\" -\", file)\n",
    "\n",
    "# Inspect one .npy file; modify the index if you prefer a different file.\n",
    "sample_file = os.path.join(output_dir, npy_files[0])\n",
    "print(\"\\nInspecting:\", sample_file)\n",
    "\n",
    "# Load the data from the selected .npy file.\n",
    "data = np.load(sample_file)\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# Calculate and print basic statistics for the loaded data.\n",
    "print(\"Statistics:\")\n",
    "print(\"   Min:\", np.nanmin(data))\n",
    "print(\"   Max:\", np.nanmax(data))\n",
    "print(\"   Mean:\", np.nanmean(data))\n",
    "print(\"   Number of NaNs:\", np.isnan(data).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045e72c-f0c5-4e41-873e-0fc6ee8a98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "\n",
    "# === FILE PATHS ===\n",
    "# Define file paths for the elevation raster, landmask, and climate data.\n",
    "elevation_path = \"elevation_resampled_to_climate.tif\"\n",
    "landmask_path = \"landmask_asia.tif\"\n",
    "climate_root = r\"C:\\Users\\FENIL\\Downloads\\climate_data\"\n",
    "ppt_path = f\"{climate_root}\\\\ppt_1990_2020\\\\TerraClimate_ppt_2009.nc\"\n",
    "tmin_path = f\"{climate_root}\\\\tmin_1990_2020\\\\TerraClimate_tmin_2009.nc\"\n",
    "tmax_path = f\"{climate_root}\\\\tmax_1990_2020\\\\TerraClimate_tmax_2009.nc\"\n",
    "\n",
    "print(\"\\nChecking raster alignment...\\n\")\n",
    "\n",
    "# === CLIMATE DATA ===\n",
    "# Load and aggregate the precipitation data by summing over the time dimension.\n",
    "print(\"Precipitation (PPT):\")\n",
    "ppt = xr.open_dataset(ppt_path)['ppt'].sum(dim='time')\n",
    "print(\" - Shape:\", ppt.shape)\n",
    "print(\" - Latitude range:\", float(ppt.lat.min()), \"to\", float(ppt.lat.max()))\n",
    "print(\" - Longitude range:\", float(ppt.lon.min()), \"to\", float(ppt.lon.max()))\n",
    "\n",
    "# Load and aggregate the minimum temperature data by averaging over time.\n",
    "print(\"\\nMinimum Temperature (TMIN):\")\n",
    "tmin = xr.open_dataset(tmin_path)['tmin'].mean(dim='time')\n",
    "print(\" - Shape:\", tmin.shape)\n",
    "\n",
    "# Load and aggregate the maximum temperature data by averaging over time.\n",
    "print(\"\\nMaximum Temperature (TMAX):\")\n",
    "tmax = xr.open_dataset(tmax_path)['tmax'].mean(dim='time')\n",
    "print(\" - Shape:\", tmax.shape)\n",
    "\n",
    "# === ELEVATION ===\n",
    "# Open the elevation raster and read the first band.\n",
    "print(\"\\nElevation Raster:\")\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "    print(\" - Shape:\", elev.shape)\n",
    "    print(\" - Bounds:\", elev_src.bounds)\n",
    "    print(\" - Resolution:\", elev_src.res)\n",
    "\n",
    "# === LANDMASK ===\n",
    "# Open the landmask raster and read the first band.\n",
    "print(\"\\nLandmask Raster:\")\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1)\n",
    "    print(\" - Shape:\", landmask.shape)\n",
    "    print(\" - Bounds:\", lm_src.bounds)\n",
    "    print(\" - Resolution:\", lm_src.res)\n",
    "\n",
    "# === SHAPE CHECK ===\n",
    "print(\"\\nShape Match Check:\")\n",
    "# Check if the shapes of the climate rasters (ppt, tmin, tmax) are identical.\n",
    "if ppt.shape == tmin.shape == tmax.shape:\n",
    "    print(\"Climate rasters match in shape.\")\n",
    "else:\n",
    "    print(\"Climate rasters have mismatched shapes.\")\n",
    "\n",
    "# Check if the elevation and landmask rasters have matching shapes.\n",
    "if elev.shape == landmask.shape:\n",
    "    print(\"Elevation and landmask shapes match.\")\n",
    "else:\n",
    "    print(\"Elevation and landmask shapes differ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb7bfc-8a69-4f5a-8274-57572a01e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_years = list(range(2019, 2025))\n",
    "future_scenarios = {2050: {\"SSP585\": \"2050585\", \"SSP245\": \"2050245\"}}\n",
    "\n",
    "for year in selected_years:\n",
    "    predict_and_plot(year, year, str(year))\n",
    "\n",
    "for future_year, scenarios in future_scenarios.items():\n",
    "    for label, suffix in scenarios.items():\n",
    "        predict_and_plot(future_year, suffix, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9437aef-2792-457a-9d77-4339a8d4e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Define the output directory where the .npy files are stored.\n",
    "output_dir = \"sdm_takin\"\n",
    "# Area per pixel (in square kilometers) at 2.5 arcmin resolution.\n",
    "pixel_area_km2 = 13.67\n",
    "# Suitability threshold for habitat.\n",
    "threshold = 0.5\n",
    "# Define the years for the analysis.\n",
    "years = list(range(2009, 2025))\n",
    "# Define the future scenario files with scenario labels.\n",
    "future = {\n",
    "    \"2050_SSP245\": \"suitability_map_2050_SSP245.npy\",\n",
    "    \"2050_SSP585\": \"suitability_map_2050_SSP585.npy\"\n",
    "}\n",
    "\n",
    "# --- STEP 1: Create Himalayan Mask using the correct lat/lon grid ---\n",
    "# Load a sample suitability map to derive the grid dimensions.\n",
    "sample_map_path = os.path.join(output_dir, \"suitability_map_2024_2024.npy\")\n",
    "suitability_sample = np.load(sample_map_path)\n",
    "lat_len, lon_len = suitability_sample.shape\n",
    "\n",
    "# Create latitude values from 90 to -90 for the grid.\n",
    "latitudes = np.linspace(90, -90, lat_len)\n",
    "# Create longitude values from -180 to 180 (fixing the default from 0–360).\n",
    "longitudes = np.linspace(-180, 180, lon_len)\n",
    "# Create meshgrids for latitude and longitude.\n",
    "lat_grid, lon_grid = np.meshgrid(latitudes, longitudes, indexing='ij')\n",
    "\n",
    "# Define the Himalayan region based on latitude and longitude boundaries:\n",
    "# Here, the region roughly covers India, Nepal, Bhutan, and southeastern Tibet.\n",
    "himalaya_mask = (\n",
    "    (lat_grid >= 25) & (lat_grid <= 38) &\n",
    "    (lon_grid >= 78) & (lon_grid <= 105)\n",
    ")\n",
    "\n",
    "print(f\"Mask shape: {himalaya_mask.shape}\")\n",
    "print(f\"Pixels in Himalayan mask: {np.count_nonzero(himalaya_mask)}\")\n",
    "print(f\"Estimated Himalayan area: {np.count_nonzero(himalaya_mask) * pixel_area_km2:,.2f} km²\")\n",
    "\n",
    "# --- STEP 2: Function to calculate suitable area using the mask ---\n",
    "def count_suitable_area(npy_path, threshold, mask):\n",
    "    \"\"\"\n",
    "    Load a suitability map from a .npy file, apply a mask, and count the area where suitability exceeds the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        npy_path (str): File path for the .npy suitability map.\n",
    "        threshold (float): Suitability threshold value.\n",
    "        mask (np.ndarray): Boolean array defining the region of interest.\n",
    "    \n",
    "    Returns:\n",
    "        float: Total suitable area in km².\n",
    "    \"\"\"\n",
    "    data = np.load(npy_path)\n",
    "    # Apply the mask to the data; pixels outside the mask become NaN.\n",
    "    masked = np.where(mask, data, np.nan)\n",
    "    # Count the number of pixels where suitability is above the threshold.\n",
    "    suitable_pixels = np.count_nonzero(masked > threshold)\n",
    "    return suitable_pixels * pixel_area_km2\n",
    "\n",
    "# --- STEP 3: Calculate area for each year ---\n",
    "area_by_year = []\n",
    "for year in years:\n",
    "    npy_file = os.path.join(output_dir, f\"suitability_map_{year}_{year}.npy\")\n",
    "    if os.path.exists(npy_file):\n",
    "        area = count_suitable_area(npy_file, threshold, himalaya_mask)\n",
    "        area_by_year.append((year, area))\n",
    "    else:\n",
    "        print(f\"Missing file: {npy_file}\")\n",
    "\n",
    "# --- STEP 4: Add future scenarios ---\n",
    "for label, filename in future.items():\n",
    "    npy_path = os.path.join(output_dir, filename)\n",
    "    if os.path.exists(npy_path):\n",
    "        area = count_suitable_area(npy_path, threshold, himalaya_mask)\n",
    "        area_by_year.append((label, area))\n",
    "    else:\n",
    "        print(f\"Missing file: {npy_path}\")\n",
    "\n",
    "# --- STEP 5: Plot the trend ---\n",
    "# Sort the results so that labels are in logical order.\n",
    "area_by_year = sorted(area_by_year, key=lambda x: str(x[0]))\n",
    "labels, areas = zip(*area_by_year)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(labels, areas, marker='o', linewidth=2, color='darkgreen', label='Himalayan Suitable Area')\n",
    "\n",
    "# Draw vertical lines to denote the future scenarios for clarity.\n",
    "if '2050_SSP245' in labels:\n",
    "    plt.axvline('2050_SSP245', color='orange', linestyle='--', label='SSP245 (2050)')\n",
    "if '2050_SSP585' in labels:\n",
    "    plt.axvline('2050_SSP585', color='red', linestyle='--', label='SSP585 (2050)')\n",
    "\n",
    "plt.title(\"Takin Habitat Area Trend in the Himalayas (Threshold > 0.5)\", fontsize=14)\n",
    "plt.ylabel(\"Suitable Area (km²)\", fontsize=12)\n",
    "plt.xlabel(\"Year / Scenario\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- STEP 6: Save and Show Plot ---\n",
    "plot_path = os.path.join(output_dir, \"takin_himalaya_area_trend.png\")\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Area trend plot saved to: {plot_path}\")\n",
    "\n",
    "# --- STEP 7: Optional - Print values year by year ---\n",
    "print(\"\\nYear-wise Habitat Area:\")\n",
    "for label, area in area_by_year:\n",
    "    print(f\"{label}: {area:,.2f} km²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0bac6-d38c-444e-98ba-346c7493b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Specify directories and file paths for output, elevation, landmask, and climate data.\n",
    "output_dir = \"sdm_takin\"\n",
    "elevation_path = r\"elevation_resampled_to_climate.tif\"\n",
    "landmask_path = r\"landmask_asia.tif\"\n",
    "ppt_dir = r\"C:\\Users\\FENIL\\Downloads\\climate_data\\ppt_1990_2020\"\n",
    "\n",
    "# List of past years to process and definitions for future scenarios.\n",
    "selected_years = list(range(2009, 2025))\n",
    "future_scenarios = {\n",
    "    \"2050_SSP245\": \"SSP245\",\n",
    "    \"2050_SSP585\": \"SSP585\"\n",
    "}\n",
    "# Suitability threshold for defining habitat.\n",
    "threshold = 0.5\n",
    "\n",
    "# --- Load Elevation and Landmask ---\n",
    "# Open the elevation raster and read the first band.\n",
    "with rasterio.open(elevation_path) as elev_src:\n",
    "    elev = elev_src.read(1)\n",
    "\n",
    "# Open the landmask raster and convert it to a boolean array (True = land).\n",
    "with rasterio.open(landmask_path) as lm_src:\n",
    "    landmask = lm_src.read(1).astype(bool)\n",
    "\n",
    "# --- Get Latitude/Longitude Grid from a Sample NetCDF File ---\n",
    "# Open one of the climate NetCDF files to retrieve latitude and longitude values.\n",
    "sample_nc = xr.open_dataset(os.path.join(ppt_dir, \"TerraClimate_ppt_2019.nc\"))\n",
    "lat_vals = sample_nc['lat'].values\n",
    "lon_vals = sample_nc['lon'].values\n",
    "# Create a meshgrid that matches the raster grid's dimensions.\n",
    "lat_grid, lon_grid = np.meshgrid(lat_vals, lon_vals, indexing='ij')\n",
    "\n",
    "# --- Function to Calculate Centroid with Outlier Trimming ---\n",
    "def compute_centroid(data, lat_grid, lon_grid, elev_map, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute the centroid of the region where suitability exceeds the threshold,\n",
    "    trimming out outlier values based on percentiles.\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray): Suitability map.\n",
    "        lat_grid (ndarray): Grid of latitude values.\n",
    "        lon_grid (ndarray): Grid of longitude values.\n",
    "        elev_map (ndarray): Elevation data aligned with the climate grid.\n",
    "        threshold (float): Suitability threshold for habitat.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean latitude, mean longitude, median elevation) of the region.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask for pixels above the suitability threshold.\n",
    "    mask = data > threshold\n",
    "    if not np.any(mask):\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Extract latitudes, longitudes, and elevation values of suitable pixels.\n",
    "    lats = lat_grid[mask]\n",
    "    lons = lon_grid[mask]\n",
    "    elevs = elev_map[mask]\n",
    "\n",
    "    # Determine the 5th and 95th percentile to trim outlier values.\n",
    "    lat_low, lat_high = np.percentile(lats, [5, 95])\n",
    "    lon_low, lon_high = np.percentile(lons, [5, 95])\n",
    "    elev_low, elev_high = np.percentile(elevs, [5, 95])\n",
    "\n",
    "    # Retain only those values that fall within the trimmed percentile range.\n",
    "    valid = (lats >= lat_low) & (lats <= lat_high) & \\\n",
    "            (lons >= lon_low) & (lons <= lon_high) & \\\n",
    "            (elevs >= elev_low) & (elevs <= elev_high)\n",
    "\n",
    "    # Compute and return the centroid (mean latitude and longitude) and median elevation.\n",
    "    return np.mean(lats[valid]), np.mean(lons[valid]), np.median(elevs[valid])\n",
    "\n",
    "# --- Collect Centroid Data for Each Scenario ---\n",
    "centroid_data = []\n",
    "\n",
    "# Loop over past years and compute centroids.\n",
    "for year in selected_years:\n",
    "    npy_path = os.path.join(output_dir, f\"suitability_map_{year}_{year}.npy\")\n",
    "    if os.path.exists(npy_path):\n",
    "        data = np.load(npy_path)\n",
    "        lat_c, lon_c, elev_c = compute_centroid(data, lat_grid, lon_grid, elev, threshold)\n",
    "        centroid_data.append((str(year), lat_c, lon_c, elev_c))\n",
    "    else:\n",
    "        print(f\"File not found: {npy_path}\")\n",
    "\n",
    "# Loop over future scenarios and compute centroids.\n",
    "for label, suffix in future_scenarios.items():\n",
    "    npy_path = os.path.join(output_dir, f\"suitability_map_2050_{suffix}.npy\")\n",
    "    if os.path.exists(npy_path):\n",
    "        data = np.load(npy_path)\n",
    "        lat_c, lon_c, elev_c = compute_centroid(data, lat_grid, lon_grid, elev, threshold)\n",
    "        centroid_data.append((label, lat_c, lon_c, elev_c))\n",
    "    else:\n",
    "        print(f\"File not found: {npy_path}\")\n",
    "\n",
    "# --- Save the Centroid Data to a CSV File ---\n",
    "# Convert the centroid data to a DataFrame, sort by the 'Year' column, and reset the index.\n",
    "centroid_df = pd.DataFrame(centroid_data, columns=[\"Year\", \"Centroid_Lat\", \"Centroid_Lon\", \"Median_Elevation\"])\n",
    "centroid_df.sort_values(\"Year\", inplace=True)\n",
    "centroid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the CSV output path and save the DataFrame.\n",
    "csv_path = os.path.join(output_dir, \"centroid_shifts_takin.csv\")\n",
    "centroid_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nTakin centroid data saved to: {csv_path}\")\n",
    "print(centroid_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae396302-5ff1-4900-a083-9c4379d3878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Define the path to the CSV file containing centroid shift data.\n",
    "csv_path = os.path.join(\"sdm_takin\", \"centroid_shifts_takin.csv\")\n",
    "# Define the output directory for saving the plot.\n",
    "output_dir = \"sdm_takin\"\n",
    "# Specify the file name and path for the trend plot.\n",
    "output_file = os.path.join(output_dir, \"centroid_shift_trends_takin.png\")\n",
    "\n",
    "# --- LOAD AND PREPARE DATA ---\n",
    "# Read the centroid shifts CSV file into a DataFrame.\n",
    "df = pd.read_csv(csv_path)\n",
    "# Create a numeric year column for proper sorting.\n",
    "# If the 'Year' value contains an underscore, split it and use the first part; otherwise, convert directly.\n",
    "df[\"Year_Num\"] = df[\"Year\"].apply(lambda x: int(x.split(\"_\")[0]) if \"_\" in x else int(x))\n",
    "# Sort the DataFrame based on the numeric year.\n",
    "df = df.sort_values(\"Year_Num\")\n",
    "\n",
    "# Create a mask to identify the future scenarios (those with '2050' in the Year field).\n",
    "future_mask = df[\"Year\"].str.contains(\"2050\", na=False)\n",
    "\n",
    "# --- PLOT THE TRENDS ---\n",
    "# Set up a figure with three subplots for latitude, longitude, and elevation.\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot centroid latitude shifts.\n",
    "axs[0].plot(df[\"Year\"], df[\"Centroid_Lat\"], marker='o', color='royalblue')\n",
    "# Highlight the 2050 scenario points with a different color.\n",
    "axs[0].scatter(df[\"Year\"][future_mask], df[\"Centroid_Lat\"][future_mask], color='black', label='2050 Scenarios')\n",
    "axs[0].set_ylabel(\"Latitude (°N)\")\n",
    "axs[0].set_title(\"Centroid Latitude Shift\")\n",
    "axs[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot centroid longitude shifts.\n",
    "axs[1].plot(df[\"Year\"], df[\"Centroid_Lon\"], marker='o', color='forestgreen')\n",
    "axs[1].scatter(df[\"Year\"][future_mask], df[\"Centroid_Lon\"][future_mask], color='black')\n",
    "axs[1].set_ylabel(\"Longitude (°E)\")\n",
    "axs[1].set_title(\"Centroid Longitude Shift\")\n",
    "axs[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot median elevation shifts.\n",
    "axs[2].plot(df[\"Year\"], df[\"Median_Elevation\"], marker='o', color='firebrick')\n",
    "axs[2].scatter(df[\"Year\"][future_mask], df[\"Median_Elevation\"][future_mask], color='black')\n",
    "axs[2].set_ylabel(\"Elevation (m)\")\n",
    "axs[2].set_title(\"Median Elevation Shift\")\n",
    "axs[2].grid(True, linestyle='--', alpha=0.5)\n",
    "axs[2].set_xlabel(\"Year / Scenario\")\n",
    "\n",
    "# Rotate x-axis labels for better readability and adjust layout.\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the resulting plot as a high-resolution PNG file.\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Inform the user that the plot has been saved.\n",
    "print(f\"Takin centroid trend plot saved: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f1feb-a059-46e8-bda1-ffcc8b34c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# --- Load the centroid shift CSV for Takin ---\n",
    "# Read the centroid shifts data which includes year, latitude, and longitude information.\n",
    "df = pd.read_csv(\"sdm_takin/centroid_shifts_takin.csv\")\n",
    "\n",
    "# --- Sort by Numeric Year ---\n",
    "# Create a numeric representation of the Year column to sort the data chronologically.\n",
    "df[\"Year_Num\"] = df[\"Year\"].apply(lambda x: int(x.split(\"_\")[0]) if \"_\" in x else int(x))\n",
    "df = df.sort_values(\"Year_Num\").reset_index(drop=True)\n",
    "\n",
    "# --- Calculate Geodesic Distances Between Consecutive Years ---\n",
    "# Initialize the list of distances. The first year has no previous record, so its shift is 0 km.\n",
    "distances_km = [0]\n",
    "# Loop over each record (starting from the second) and compute the geodesic distance from the previous year's centroid.\n",
    "for i in range(1, len(df)):\n",
    "    prev = (df.loc[i - 1, \"Centroid_Lat\"], df.loc[i - 1, \"Centroid_Lon\"])\n",
    "    curr = (df.loc[i, \"Centroid_Lat\"], df.loc[i, \"Centroid_Lon\"])\n",
    "    # Calculate the distance in kilometers using geopy's geodesic method.\n",
    "    distance = geodesic(prev, curr).kilometers\n",
    "    distances_km.append(distance)\n",
    "\n",
    "# Add the calculated distances as a new column in the DataFrame.\n",
    "df[\"Distance_Change_km\"] = distances_km\n",
    "\n",
    "# --- Save the Updated DataFrame ---\n",
    "# Define the output CSV file path.\n",
    "output_path = os.path.join(\"sdm_takin\", \"centroid_distance_changes_takin.csv\")\n",
    "# Write the updated DataFrame to the CSV file without the index.\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print a confirmation message along with key columns to verify the results.\n",
    "print(f\"Distance shift table saved: {output_path}\")\n",
    "print(df[[\"Year\", \"Centroid_Lat\", \"Centroid_Lon\", \"Distance_Change_km\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a6268-3295-4416-853d-41303dd9b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# --- Load the centroid shift CSV for Takin ---\n",
    "# Read the CSV file that contains the centroid shift data (year, latitude, longitude).\n",
    "df = pd.read_csv(\"sdm_takin/centroid_shifts_takin.csv\")\n",
    "\n",
    "# --- Sort by Numeric Year ---\n",
    "# Create a numeric version of the \"Year\" column. If the year contains an underscore (e.g. for future scenarios),\n",
    "# split the string and use the numeric part only. Otherwise, convert directly to integer.\n",
    "df[\"Year_Num\"] = df[\"Year\"].apply(lambda x: int(x.split(\"_\")[0]) if \"_\" in x else int(x))\n",
    "# Sort the DataFrame based on the numeric year and reset the index.\n",
    "df = df.sort_values(\"Year_Num\").reset_index(drop=True)\n",
    "\n",
    "# --- Calculate Geodesic Distances Between Consecutive Years ---\n",
    "# Initialize a list to store the distance changes (in km). The first year is set to 0 as a starting point.\n",
    "distances_km = [0]\n",
    "# Loop through the DataFrame starting from the second row and calculate the geodesic distance between consecutive centroids.\n",
    "for i in range(1, len(df)):\n",
    "    # Get the centroid coordinates (latitude, longitude) for the previous year.\n",
    "    prev = (df.loc[i - 1, \"Centroid_Lat\"], df.loc[i - 1, \"Centroid_Lon\"])\n",
    "    # Get the centroid coordinates for the current year.\n",
    "    curr = (df.loc[i, \"Centroid_Lat\"], df.loc[i, \"Centroid_Lon\"])\n",
    "    # Calculate the geodesic distance between these two centroids in kilometers.\n",
    "    distance = geodesic(prev, curr).kilometers\n",
    "    distances_km.append(distance)\n",
    "\n",
    "# Add the list of calculated distance changes as a new column in the DataFrame.\n",
    "df[\"Distance_Change_km\"] = distances_km\n",
    "\n",
    "# --- Save the Updated DataFrame ---\n",
    "# Define the output file path for saving the updated data.\n",
    "output_path = os.path.join(\"sdm_takin\", \"centroid_distance_changes_takin.csv\")\n",
    "# Save the DataFrame to a CSV file without including the index.\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print a confirmation message along with selected columns to verify the results.\n",
    "print(f\"Distance shift table saved: {output_path}\")\n",
    "print(df[[\"Year\", \"Centroid_Lat\", \"Centroid_Lon\", \"Distance_Change_km\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
